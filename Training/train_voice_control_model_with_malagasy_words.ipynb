{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO4-CY_TCZZS"
      },
      "source": [
        "# Train a Audio Recognition Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaFfr7DHRmGF"
      },
      "source": [
        "This notebook demonstrates how to train a Audio Recognition model to recognize keywords in speech.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaVtYN4nlCft"
      },
      "source": [
        "\n",
        "**Training is much faster using GPU acceleration.** Before you proceed, ensure you are using a GPU runtime by going to **Runtime -> Change runtime type** and set **Hardware accelerator: GPU**. Training 15,000 iterations will take 1.5 - 2 hours on a GPU runtime.\n",
        "\n",
        "## Configure Defaults\n",
        "\n",
        "**MODIFY** the following constants for your specific use case."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6gdYu79uh-Cv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ludfxbNIaegy"
      },
      "outputs": [],
      "source": [
        "# The objective is to control light using the Malagasy language.\n",
        "# Provide a comma-separated list of the words you want to train.\n",
        "# All other words in the dataset will be used to train an \"unknown\" label,\n",
        "#while silent audio data with no spoken words will be used to train a \"silence\" label.\n",
        "\n",
        "# You will upload audio files in the next cell\n",
        "WANTED_WORDS = \"mirehitra,maty\"\n",
        "# Mirehitra means light on and Maty means light off\n",
        "\n",
        "#The number of training steps and learning rates can be specified as\n",
        "#comma-separated lists to define the rate at each stage. For example, if you\n",
        "#set TRAINING_STEPS=12000,3000 and LEARNING_RATE=0.001,0.0001, the model will\n",
        "#run a total of 15,000 training loops.\n",
        "#It will use a learning rate of 0.001 for the first 12,000 steps and a rate of\n",
        "#0.0001 for the final 3,000 steps. If you select TRAINING_STEPS=12000,3000,\n",
        "#it will take approximately 2 hours of training.\n",
        "TRAINING_STEPS = \"5000,1500\"\n",
        "LEARNING_RATE = \"0.001,0.0001\"\n",
        "\n",
        "# Calculate the total number of steps, which is used to identify the checkpoint\n",
        "# file name.\n",
        "TOTAL_STEPS = str(sum(map(lambda string: int(string), TRAINING_STEPS.split(\",\"))))\n",
        "\n",
        "# Print the configuration to confirm it\n",
        "print(\"Training these words: %s\" % WANTED_WORDS)\n",
        "print(\"Training steps in each stage: %s\" % TRAINING_STEPS)\n",
        "print(\"Learning rate in each stage: %s\" % LEARNING_RATE)\n",
        "print(\"Total number of training steps: %s\" % TOTAL_STEPS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCgeOpvY9pAi"
      },
      "source": [
        "The following constants as they include filepaths used in this notebook and data that is shared during training and inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nd1iM1o2ymvA"
      },
      "outputs": [],
      "source": [
        "# Calculate the percentage of 'silence' and 'unknown' training samples required\n",
        "# to ensure that we have equal number of samples for each label.\n",
        "number_of_labels = WANTED_WORDS.count(',') + 1\n",
        "number_of_total_labels = number_of_labels + 2 # for 'silence' and 'unknown' label\n",
        "equal_percentage_of_training_samples = int(100.0/(number_of_total_labels))\n",
        "SILENT_PERCENTAGE = equal_percentage_of_training_samples\n",
        "UNKNOWN_PERCENTAGE = equal_percentage_of_training_samples\n",
        "\n",
        "# Constants which are shared during training and inference\n",
        "PREPROCESS = 'micro'\n",
        "WINDOW_STRIDE = 20\n",
        "MODEL_ARCHITECTURE = 'tiny_conv' # Other options include: single_fc, conv,\n",
        "                      # low_latency_conv, low_latency_svdf, tiny_embedding_conv\n",
        "\n",
        "# Constants used during training only\n",
        "VERBOSITY = 'WARN'\n",
        "EVAL_STEP_INTERVAL = '500'\n",
        "SAVE_STEP_INTERVAL = '500'\n",
        "\n",
        "# Constants for training directories and filepaths\n",
        "DATASET_DIR =  'dataset/'\n",
        "LOGS_DIR = 'logs/'\n",
        "TRAIN_DIR = 'train/' # for training checkpoints and other files.\n",
        "\n",
        "# Constants for inference directories and filepaths\n",
        "import os\n",
        "MODELS_DIR = 'models'\n",
        "if not os.path.exists(MODELS_DIR):\n",
        "  os.mkdir(MODELS_DIR)\n",
        "MODEL_TF = os.path.join(MODELS_DIR, 'model.pb')\n",
        "MODEL_TFLITE = os.path.join(MODELS_DIR, 'model.tflite')\n",
        "FLOAT_MODEL_TFLITE = os.path.join(MODELS_DIR, 'float_model.tflite')\n",
        "MODEL_TFLITE_MICRO = os.path.join(MODELS_DIR, 'model.cc')\n",
        "SAVED_MODEL = os.path.join(MODELS_DIR, 'saved_model')\n",
        "\n",
        "QUANT_INPUT_MIN = 0.0\n",
        "QUANT_INPUT_MAX = 26.0\n",
        "QUANT_INPUT_RANGE = QUANT_INPUT_MAX - QUANT_INPUT_MIN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QpT5x5sL0RA"
      },
      "source": [
        "## Import your Custom Dataset\n",
        "To begin, the algorithme will download Gonzalez's dataset, which will serve as a foundation of \"unkown\" and \"silent\" label upon which you can construct your own dataset. You can access the dataset on [Google Drive](https://drive.google.com/file/d/1hpYc-YnBCHDhu2M4uvbIFDBI6CCTWjvw/view?pli=1).\n",
        "\n",
        "This strategy will significantly enhance your model's performance, especially if you're training with a limited amount of data. This approach will influence the results.\n",
        "\n",
        "\n",
        "Gonzalez's dataset consists of:\n",
        "\n",
        "*   environmental background noise, such as rain and wind. The \"silent\" label is trained using this data.\n",
        "*   additionaly, there are other data representing some conversation and malagasy words to train the \"unkown\" label."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This cell dowload the dataset available in Google Drive to your\n",
        "#Google Colab Files content\n",
        "# Installation needed\n",
        "!pip install gdown\n",
        "\n",
        "import gdown\n",
        "\n",
        "# File ID from the Google Drive link\n",
        "file_id = \"1hpYc-YnBCHDhu2M4uvbIFDBI6CCTWjvw\"\n",
        "\n",
        "# Construct the download URL\n",
        "url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "\n",
        "# Specify the output file name\n",
        "output = \"dataset.rar\"\n",
        "\n",
        "# Download the file\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "print(f\"File downloaded and saved as '{output}' in Google Colab's file system.\")"
      ],
      "metadata": {
        "id": "yXGtv_oqiOxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This cell extracte the content of dataset.rar and\n",
        "#creat a directory to store the dataset\n",
        "\n",
        "# Install required libraries\n",
        "!pip install patool\n",
        "!sudo apt-get install unrar  # Install unrar for handling .rar files\n",
        "\n",
        "import patoolib\n",
        "\n",
        "# Path to the .rar file\n",
        "rar_file = \"dataset.rar\"\n",
        "\n",
        "# Directory to extract the contents\n",
        "extract_dir = \"dataset\"\n",
        "\n",
        "# Extract the .rar file\n",
        "patoolib.extract_archive(rar_file, outdir=extract_dir)\n",
        "\n",
        "print(f\"File '{rar_file}' extracted to '{extract_dir}'.\")\n",
        "\n",
        "# Verify the extracted files\n",
        "!ls -R {extract_dir}"
      ],
      "metadata": {
        "id": "fXis1eJcgEh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you'll need to upload your all of your custom *.ogg audio files that you recorded using the [Open Speech Recording tool](https://tinyml.seas.harvard.edu/open_speech_recording/).\n",
        "Please use the Google Chrome browser on a laptop. It only downloads 10 files at once, so refresh the site and repeat the process until you have sufficient datasets.\n",
        "\n",
        "Once you have finished, you can select multiple files and upload them all simultaneously! Ensure you have a high-speed internet connection.\n"
      ],
      "metadata": {
        "id": "clPTri1CiR2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploading the datasets with your wanted words.\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "5jGTX-GciTkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we can convert them into correctly trimmed WAV files and then store them in the appropriate folders in the DATASET_DIR. We will use Pete's extract_loudest_section tool which you can find more documentation about here: https://github.com/petewarden/extract_loudest_section"
      ],
      "metadata": {
        "id": "5H-zDR3TiatI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the ogg files to wavs\n",
        "!mkdir wavs\n",
        "!find *.ogg -print0 | xargs -0 basename -s .ogg | xargs -I {} ffmpeg -i {}.ogg -ar 16000 wavs/{}.wav\n",
        "!rm -r -f *.ogg\n",
        "\n",
        "# then use pete's tool to only extract 1 second clips from them for use with the KWS pipeline\n",
        "!mkdir trimmed_wavs\n",
        "!git clone https://github.com/petewarden/extract_loudest_section.git\n",
        "!make -C extract_loudest_section/\n",
        "!/tmp/extract_loudest_section/gen/bin/extract_loudest_section 'wavs/*.wav' trimmed_wavs/\n",
        "!rm -r -f /wavs"
      ],
      "metadata": {
        "id": "Yg-uEWxRiecM",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "\n",
        "# Store them in the appropriate folders\n",
        "data_index = {}\n",
        "print(\"Before:\", os.getcwd())  # Prints the current working directory\n",
        "os.chdir('/content/trimmed_wavs')  # Changes the directory\n",
        "print(\"After:\", os.getcwd())  # Prints the updated working directory\n",
        "\n",
        "DATASET_DIR =  'dataset/'\n",
        "search_path = os.path.join('*.wav')\n",
        "for wav_path in glob.glob(search_path):\n",
        "    original_wav_path = wav_path\n",
        "    parts = wav_path.split('_')\n",
        "    if len(parts) > 2:\n",
        "        wav_path = parts[0] + '_' + ''.join(parts[1:])\n",
        "    matches = re.search('([^/_]+)_([^/_]+)\\.wav', wav_path)\n",
        "    if not matches:\n",
        "        raise Exception('File name not in a recognized form:\"%s\"' % wav_path)\n",
        "    word = matches.group(1).lower()\n",
        "    instance = matches.group(2).lower()\n",
        "    if not word in data_index:\n",
        "      data_index[word] = {}\n",
        "    if instance in data_index[word]:\n",
        "        raise Exception('Audio instance already seen:\"%s\"' % wav_path)\n",
        "    data_index[word][instance] = original_wav_path\n",
        "\n",
        "output_dir = os.path.join('..', 'dataset')\n",
        "try:\n",
        "    os.mkdir(output_dir)\n",
        "except:\n",
        "    pass\n",
        "for word in data_index:\n",
        "  word_dir = os.path.join(output_dir, word)\n",
        "  try:\n",
        "      os.mkdir(word_dir)\n",
        "      print('Created dir: ' + word_dir)\n",
        "  except:\n",
        "      print('Storing in existing dir: ' + word_dir)\n",
        "  for instance in data_index[word]:\n",
        "    wav_path = data_index[word][instance]\n",
        "    output_path = os.path.join(word_dir, instance + '.wav')\n",
        "    shutil.copyfile(wav_path, output_path)\n",
        "os.chdir('..')\n",
        "!rm -r -f trimmed_wavs"
      ],
      "metadata": {
        "id": "oofOU9ZSifPF",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rLYpvtg9P4o"
      },
      "source": [
        "## Setup Environment\n",
        "\n",
        "All the datasets are ready. Now, let's install dependencies for the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed_XpUrU5DvY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfEUlfFBizio"
      },
      "source": [
        "Clone the TensorFlow Github Repository, which contains the relevant code required to run the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZArmzT85SLq"
      },
      "outputs": [],
      "source": [
        "!git clone -q --depth 1 https://github.com/tensorflow/tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS9swHLSi7Bi"
      },
      "source": [
        "Load TensorBoard to visualize the accuracy and loss as training proceeds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4qF1VxP3UE4"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {LOGS_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1J96Ron-O4R"
      },
      "source": [
        "## Training\n",
        "\n",
        "The following script begin training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJsEZx6lynbY"
      },
      "outputs": [],
      "source": [
        "!python tensorflow/tensorflow/examples/speech_commands/train.py \\\n",
        "--data_dir={DATASET_DIR} \\\n",
        "--wanted_words={WANTED_WORDS} \\\n",
        "--data_url=\"\"\\\n",
        "--background_volume=0.4\\\n",
        "--background_frequency=0.45\\\n",
        "#The parameters background_volume and background_frequency are optional.\n",
        "#They are used for data augmentation. Specifically, these values indicate that\n",
        "#the dataset is mixed with 40% background volume and that these changes are\n",
        "#applied to 45% of the dataset.\n",
        "#Since they are optional, we can remove them if desired.\n",
        "--silence_percentage={SILENT_PERCENTAGE} \\\n",
        "--unknown_percentage={UNKNOWN_PERCENTAGE} \\\n",
        "--preprocess={PREPROCESS} \\\n",
        "--window_stride={WINDOW_STRIDE} \\\n",
        "--model_architecture={MODEL_ARCHITECTURE} \\\n",
        "--how_many_training_steps={TRAINING_STEPS} \\\n",
        "--learning_rate={LEARNING_RATE} \\\n",
        "--train_dir={TRAIN_DIR} \\\n",
        "--summaries_dir={LOGS_DIR} \\\n",
        "--verbosity={VERBOSITY} \\\n",
        "--eval_step_interval={EVAL_STEP_INTERVAL} \\\n",
        "--save_step_interval={SAVE_STEP_INTERVAL}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQUJLrdS-ftl"
      },
      "source": [
        "## Generate a TensorFlow Model for Inference\n",
        "\n",
        "Combine relevant training results (graph, weights, etc) into a single file for inference. This process is known as freezing a model and the resulting model is known as a frozen model/graph, as it cannot be further re-trained after this process.\n",
        "The saved TensorFlow models are located in the models/ directory on your Google Colab Files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyc3_eLh9sAg"
      },
      "outputs": [],
      "source": [
        "!rm -rf {SAVED_MODEL}\n",
        "!python tensorflow/tensorflow/examples/speech_commands/freeze.py \\\n",
        "--wanted_words=$WANTED_WORDS \\\n",
        "--window_stride_ms=$WINDOW_STRIDE \\\n",
        "--preprocess=$PREPROCESS \\\n",
        "--model_architecture=$MODEL_ARCHITECTURE \\\n",
        "--start_checkpoint=$TRAIN_DIR$MODEL_ARCHITECTURE'.ckpt-'{TOTAL_STEPS} \\\n",
        "--save_format=saved_model \\\n",
        "--output_file={SAVED_MODEL}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DBGDxVI-nKG"
      },
      "source": [
        "## Generate a TensorFlow Lite Model\n",
        "\n",
        "Convert the frozen graph to a fully quantized TensorFlow Lite model for embedded devices. The following cell will print the model size, expected to be under 20 kilobytes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIitkqvGWmre"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "#This path is includes to enable the import of speech processing modules.\n",
        "sys.path.append(\"/content/tensorflow/tensorflow/examples/speech_commands/\")\n",
        "import input_data\n",
        "import models\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzqECqMxgBh4"
      },
      "outputs": [],
      "source": [
        "SAMPLE_RATE = 16000\n",
        "CLIP_DURATION_MS = 1000\n",
        "WINDOW_SIZE_MS = 30.0\n",
        "FEATURE_BIN_COUNT = 40\n",
        "BACKGROUND_FREQUENCY = 0.45\n",
        "BACKGROUND_VOLUME_RANGE = 0.4\n",
        "TIME_SHIFT_MS = 100.0\n",
        "\n",
        "DATA_URL = ''\n",
        "#if you use your own dataset, the DATA_URL is empty\n",
        "VALIDATION_PERCENTAGE = 10\n",
        "TESTING_PERCENTAGE = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNQdAplJV1fz"
      },
      "outputs": [],
      "source": [
        "model_settings = models.prepare_model_settings(\n",
        "    len(input_data.prepare_words_list(WANTED_WORDS.split(','))),\n",
        "    SAMPLE_RATE, CLIP_DURATION_MS, WINDOW_SIZE_MS,\n",
        "    WINDOW_STRIDE, FEATURE_BIN_COUNT, PREPROCESS)\n",
        "audio_processor = input_data.AudioProcessor(\n",
        "    DATA_URL, DATASET_DIR,\n",
        "    SILENT_PERCENTAGE, UNKNOWN_PERCENTAGE,\n",
        "    WANTED_WORDS.split(','), VALIDATION_PERCENTAGE,\n",
        "    TESTING_PERCENTAGE, model_settings, LOGS_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBj_AyCh1cC0"
      },
      "outputs": [],
      "source": [
        "with tf.compat.v1.Session() as sess:\n",
        "  float_converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL)\n",
        "  float_tflite_model = float_converter.convert()\n",
        "  float_tflite_model_size = open(FLOAT_MODEL_TFLITE, \"wb\").write(float_tflite_model)\n",
        "  print(\"Float model is %d bytes\" % float_tflite_model_size)\n",
        "\n",
        "  converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL)\n",
        "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "  converter.inference_input_type = tf.int8\n",
        "  converter.inference_output_type = tf.int8\n",
        "  def representative_dataset_gen():\n",
        "    for i in range(75):\n",
        "      data, _ = audio_processor.get_data(1, i*1, model_settings,\n",
        "                                         BACKGROUND_FREQUENCY,\n",
        "                                         BACKGROUND_VOLUME_RANGE,\n",
        "                                         TIME_SHIFT_MS,\n",
        "                                         'testing',\n",
        "                                         sess)\n",
        "      flattened_data = np.array(data.flatten(), dtype=np.float32).reshape(1, 1960)\n",
        "      yield [flattened_data]\n",
        "      print(i) # If there is an error during this cell's run, change\n",
        "      # the value in the for loop by the last value printed\n",
        "  converter.representative_dataset = representative_dataset_gen\n",
        "  tflite_model = converter.convert()\n",
        "  tflite_model_size = open(MODEL_TFLITE, \"wb\").write(tflite_model)\n",
        "  print(\"Quantized model is %d bytes\" % tflite_model_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeLiDZTbLkzv"
      },
      "source": [
        "## Testing the TensorFlow Lite model's accuracy\n",
        "\n",
        "Verify that the model we've exported is still accurate, using the TF Lite Python API and our test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQsEteKRLryJ"
      },
      "outputs": [],
      "source": [
        "# Helper function to run inference\n",
        "def run_tflite_inference(tflite_model_path, model_type=\"Float\"):\n",
        "  # Load test data\n",
        "  np.random.seed(0) # set random seed for reproducible test results.\n",
        "  with tf.compat.v1.Session() as sess:\n",
        "    test_data, test_labels = audio_processor.get_data(\n",
        "        -1, 0, model_settings, BACKGROUND_FREQUENCY, BACKGROUND_VOLUME_RANGE,\n",
        "        TIME_SHIFT_MS, 'testing', sess)\n",
        "  test_data = np.expand_dims(test_data, axis=1).astype(np.float32)\n",
        "\n",
        "  # Initialize the interpreter\n",
        "  interpreter = tf.lite.Interpreter(tflite_model_path,\n",
        "                                    experimental_op_resolver_type=tf.lite.experimental.OpResolverType.BUILTIN_REF)\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  output_details = interpreter.get_output_details()[0]\n",
        "\n",
        "  # For quantized models, manually quantize the input data from float to integer\n",
        "  if model_type == \"Quantized\":\n",
        "    input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "    test_data = test_data / input_scale + input_zero_point\n",
        "    test_data = test_data.astype(input_details[\"dtype\"])\n",
        "\n",
        "  correct_predictions = 0\n",
        "  for i in range(len(test_data)):\n",
        "    interpreter.set_tensor(input_details[\"index\"], test_data[i])\n",
        "    interpreter.invoke()\n",
        "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
        "    top_prediction = output.argmax()\n",
        "    correct_predictions += (top_prediction == test_labels[i])\n",
        "\n",
        "  print('%s model accuracy is %f%% (Number of test samples=%d)' % (\n",
        "      model_type, (correct_predictions * 100) / len(test_data), len(test_data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-pD52Na6jRa"
      },
      "outputs": [],
      "source": [
        "# Compute float model accuracy\n",
        "run_tflite_inference(FLOAT_MODEL_TFLITE)\n",
        "\n",
        "# Compute quantized model accuracy\n",
        "run_tflite_inference(MODEL_TFLITE, model_type='Quantized')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt6Zqbxu-wIi"
      },
      "source": [
        "## Generate a TensorFlow Lite for MicroControllers Model\n",
        "Convert the TensorFlow Lite model into a C source file that can be loaded by TensorFlow Lite for Microcontrollers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XohZOTjR8ZyE"
      },
      "outputs": [],
      "source": [
        "# Install xxd if it is not available\n",
        "!apt-get update && apt-get -qq install xxd\n",
        "# Convert to a C source file\n",
        "!xxd -i {MODEL_TFLITE} > {MODEL_TFLITE_MICRO}\n",
        "# Update variable names\n",
        "REPLACE_TEXT = MODEL_TFLITE.replace('/', '_').replace('.', '_')\n",
        "!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {MODEL_TFLITE_MICRO}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pQnN0i_-0L2"
      },
      "source": [
        "## Deploy to a Microcontroller\n",
        "The model was deployed on Arduino Nano 33 BLE Sense Rev2 board.\n",
        "\n",
        "First, download the model.cc file located in the models/ directory. Open the Arduino IDE and make sure you have installed all the libraries to use the appropriate board. Install the [TensorFlow Lite Micro Library for Arduino](https://github.com/spaziochirale/Chirale_TensorFlowLite.git), and in the libraries installed, open the micro_speech example. Follow these instructions:\n",
        "\n",
        "1. Open the micro_features_model.cpp file and change the raw data to the new one in model.cc. It represents the AI model created to perform the tasks.\n",
        "2. Change the kCategoryLabels in micro_features_micro_model_settings.cpp file. Change the yes and no to your desired words; in this case, \"mirehatra,\" \"maty.\"\n",
        "3. Download the float_model.tflite model and visualize the model structure using [Netron](https://netron.app/). Then modify your operators used to run the model in the .ino file."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data processing"
      ],
      "metadata": {
        "id": "4WuKO1jEdG25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section is don't need to be executed for the training.\n",
        "\n",
        "The code below slices uploaded WAV files into one-second clips and compresses all resulting files into a ZIP archive. It was used to prepare the unknown dataset."
      ],
      "metadata": {
        "id": "vpildmHwdM5U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Slicing the WAV file"
      ],
      "metadata": {
        "id": "O4GY4wQBeYbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from google.colab import files\n",
        "\n",
        "# Create a directory to store sliced audio files\n",
        "output_dir = \"/content/sliced_audio\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Upload WAV files\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith(\".wav\"):\n",
        "        # Load audio file\n",
        "        filepath = os.path.join(\"/content\", filename)\n",
        "        audio, sr = librosa.load(filepath, sr=None)  # Keep original sample rate\n",
        "\n",
        "        # Get duration and number of 1-second slices\n",
        "        duration = librosa.get_duration(y=audio, sr=sr)\n",
        "        num_slices = int(duration)\n",
        "\n",
        "        for i in range(num_slices):\n",
        "            start_sample = i * sr  # Start sample index\n",
        "            end_sample = (i + 1) * sr  # End sample index\n",
        "\n",
        "            # Slice audio\n",
        "            sliced_audio = audio[start_sample:end_sample]\n",
        "\n",
        "            # Save the sliced audio\n",
        "            slice_filename = f\"{os.path.splitext(filename)[0]}_{i+1}.wav\"\n",
        "            slice_path = os.path.join(output_dir, slice_filename)\n",
        "            sf.write(slice_path, sliced_audio, sr)\n",
        "\n",
        "        print(f\"Processed: {filename} -> {num_slices} slices saved in {output_dir}\")\n",
        "\n",
        "print(\"Slicing complete! You can download the files from the 'Files' tab in Colab.\")"
      ],
      "metadata": {
        "id": "9FZUQsEfeuQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating a ZIP file"
      ],
      "metadata": {
        "id": "NsbwMbtne0jG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "output_dir = \"/content/sliced_audio\"\n",
        "zip_path = \"/content/sliced_audio.zip\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Create a ZIP file containing all sliced audio files\n",
        "with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
        "    for root, _, files in os.walk(output_dir):\n",
        "        for file in files:\n",
        "            zipf.write(os.path.join(root, file), file)\n",
        "\n",
        "print(f\"ZIP file created: {zip_path}\")"
      ],
      "metadata": {
        "id": "40FX6udHjF5W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "22cb1d09959a40fdc50ccd77b5464bb60602aea13b58d7f13d7eaffcd0bc7c7d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}